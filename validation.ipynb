{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b3be1c",
   "metadata": {},
   "source": [
    "# Validation notebook for MHNet\n",
    "# This cell is a short description of the notebook.\n",
    "\n",
    "# Title: MHNet Validation\n",
    "# Description: Load a trained MHNet checkpoint and run validation on COD10K-v2 Test set.\n",
    "# The notebook computes IoU, Dice/F1, MAE, saves per-image metrics to CSV and visualizes predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26543451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Test images: /home/ubd/23b6034/MHNet/dasatet/COD10k-v2/Test/Images/Image\n",
      "Checkpoint: models/first600/best.pth\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Import Dependencies and Setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Project imports (your repo)\n",
    "from danial import model as mh_model\n",
    "from danial import dataloader\n",
    "\n",
    "# Configurable paths and parameters\n",
    "DATA_ROOT = '/home/ubd/23b6034/MHNet/dasatet/COD10k-v2'\n",
    "TEST_IMAGE_DIR = os.path.join(DATA_ROOT, 'Test/Images/Image')\n",
    "TEST_MASK_DIR = os.path.join(DATA_ROOT, 'Test/GT_Objects/GT_Object')\n",
    "MODEL_DIR = 'models/first600'  # change if needed\n",
    "CHECKPOINT = os.path.join(MODEL_DIR, 'best.pth')  # default checkpoint\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 8\n",
    "THRESH = 0.5\n",
    "SAVE_RESULTS_DIR = Path('results/validation_first600')\n",
    "(SAVE_RESULTS_DIR / 'images').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Device:', DEVICE)\n",
    "print('Test images:', TEST_IMAGE_DIR)\n",
    "print('Checkpoint:', CHECKPOINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea77e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubd/23b6034/MHNet/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubd/23b6034/MHNet/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded keys from checkpoint: ['model_state_dict', 'seg_head_state_dict']\n",
      "Checkpoint epoch: 264\n",
      "Checkpoint loss: 0.03108286906339736\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Load Trained Model and Segmentation Head\n",
    "\n",
    "# Initialize model and seg_head (same as train.py)\n",
    "net = mh_model.Model().to(DEVICE)\n",
    "\n",
    "seg_head = nn.Sequential(\n",
    "    nn.Conv2d(64, 1, kernel_size=1),\n",
    "    nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load checkpoint\n",
    "if os.path.exists(CHECKPOINT):\n",
    "    ckpt = torch.load(CHECKPOINT, map_location=DEVICE)\n",
    "    loaded = []\n",
    "    # Try several common key names\n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'model_state_dict' in ckpt:\n",
    "            net.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "            loaded.append('model_state_dict')\n",
    "        else:\n",
    "            try:\n",
    "                net.load_state_dict(ckpt, strict=False)\n",
    "                loaded.append('direct_state_dict')\n",
    "            except Exception:\n",
    "                pass\n",
    "        if 'seg_head_state_dict' in ckpt:\n",
    "            seg_head.load_state_dict(ckpt['seg_head_state_dict'], strict=False)\n",
    "            loaded.append('seg_head_state_dict')\n",
    "    else:\n",
    "        # ckpt may be a raw state_dict\n",
    "        try:\n",
    "            net.load_state_dict(ckpt, strict=False)\n",
    "            loaded.append('raw_state_dict')\n",
    "        except Exception as e:\n",
    "            print('Failed to load checkpoint:', e)\n",
    "    print('Loaded keys from checkpoint:', loaded)\n",
    "    if 'epoch' in ckpt:\n",
    "        print('Checkpoint epoch:', ckpt.get('epoch'))\n",
    "    if 'loss' in ckpt:\n",
    "        print('Checkpoint loss:', ckpt.get('loss'))\n",
    "else:\n",
    "    print('WARNING: checkpoint not found at', CHECKPOINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0fb39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added size handling to PreFCS\n"
     ]
    }
   ],
   "source": [
    "# Add size handling fix to PreFCS\n",
    "def fix_pre_fcs_sizes(model):\n",
    "    \"\"\"Monkey patch PreFCS to handle size mismatches\"\"\"\n",
    "    original_forward = model.pre_fcs.forward\n",
    "    \n",
    "    def new_forward(self, feature3, prm_out):\n",
    "        # 1. Reduce feature3 channels\n",
    "        f3 = self.feature3_conv(feature3)  # [B, 64, H, W]\n",
    "        \n",
    "        # 2. Handle PRM output format\n",
    "        if prm_out.dim() == 5:  # [B, 1, C, H, W]\n",
    "            prm_out_4d = prm_out.squeeze(1)  # [B, C, H, W]\n",
    "        else:\n",
    "            prm_out_4d = prm_out\n",
    "            \n",
    "        # 3. Force both to target size (224,224)\n",
    "        f3_up = F.interpolate(f3, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        prm_up = F.interpolate(prm_out_4d, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 4. Element-wise addition (now sizes match)\n",
    "        fused = f3_up + prm_up\n",
    "        self.branch = fused\n",
    "        \n",
    "        # Rest of the pipeline\n",
    "        attn_out = self.attention(fused)\n",
    "        rpn_out = self.rpn(attn_out)\n",
    "        roi_input = attn_out + rpn_out\n",
    "        return roi_input\n",
    "    \n",
    "    # Replace the forward method\n",
    "    import types\n",
    "    model.pre_fcs.forward = types.MethodType(new_forward, model.pre_fcs)\n",
    "    return model\n",
    "\n",
    "# Apply the fix to our model\n",
    "net = fix_pre_fcs_sizes(net)\n",
    "print(\"Added size handling to PreFCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d21636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 image-mask pairs\n",
      "Validation samples: 4000\n"
     ]
    }
   ],
   "source": [
    "# Section 3: Create Validation Dataset\n",
    "\n",
    "# Build the dataset using the same helper as training\n",
    "val_dataset = dataloader.SegmentationDataset(\n",
    "    image_dir=TEST_IMAGE_DIR,\n",
    "    mask_dir=TEST_MASK_DIR,\n",
    "    target_size=(224, 224),\n",
    "    normalize_imagenet=True\n",
    ")\n",
    "\n",
    "val_loader = val_dataset.get_dataloader(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print('Validation samples:', len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217aff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Inference helper and utilities\n",
    "\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"Convert a torch tensor (C,H,W) or numpy array to uint8 HxWx3.\n",
    "\n",
    "    Handles:\n",
    "    - torch.Tensor with shape (C,H,W) or (1,C,H,W) or (B,C,H,W) (will take first sample)\n",
    "    - numpy arrays with channel-first or channel-last ordering.\n",
    "    Raises a helpful error if shape is unexpected.\n",
    "    \"\"\"\n",
    "    # Convert torch -> numpy\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        arr = tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.array(tensor)\n",
    "\n",
    "    # If batch dimension present, take first sample\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[0]\n",
    "\n",
    "    # If channel-first expected: (C,H,W)\n",
    "    if arr.ndim == 3 and arr.shape[0] in (1, 3):\n",
    "        c, h, w = arr.shape\n",
    "        img = arr.transpose(1, 2, 0)\n",
    "    # If channel-last already: (H,W,3) or (H,W,1)\n",
    "    elif arr.ndim == 3 and arr.shape[2] in (1, 3):\n",
    "        img = arr\n",
    "        h, w, c = img.shape\n",
    "        if c == 1:\n",
    "            img = np.repeat(img, 3, axis=2)\n",
    "    else:\n",
    "        raise ValueError(f\"tensor_to_image: unsupported input shape {arr.shape}. Expected (C,H,W) or (H,W,3).\")\n",
    "\n",
    "    # If values are normalized (small floats), unnormalize by ImageNet stats\n",
    "    if img.dtype == np.float32 or img.dtype == np.float64:\n",
    "        # assume image is normalized in channel-last float in [0,1] or standardized\n",
    "        try:\n",
    "            # Try to reverse ImageNet std/mean if values look standardized (mean near 0)\n",
    "            if np.abs(img.mean()) < 2.0:\n",
    "                img = (img * IMAGENET_STD + IMAGENET_MEAN)\n",
    "        except Exception:\n",
    "            pass\n",
    "        img = np.clip(img * 255.0, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_mask_image(mask_np, outpath):\n",
    "    \"\"\"Save a binary or float mask as a grayscale PNG.\n",
    "\n",
    "    Accepts mask in shapes: (H,W), (1,H,W), (H,W,1), or boolean.\n",
    "    \"\"\"\n",
    "    arr = np.array(mask_np)\n",
    "    # Reduce extra dims\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[0]\n",
    "    if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "        arr = arr[0]\n",
    "    if arr.ndim == 3 and arr.shape[2] == 1:\n",
    "        arr = arr[..., 0]\n",
    "\n",
    "    # Normalize floats to 0-255\n",
    "    if arr.dtype == np.bool_:\n",
    "        img = (arr.astype(np.uint8) * 255)\n",
    "    elif np.issubdtype(arr.dtype, np.floating):\n",
    "        img = (np.clip(arr, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "    else:\n",
    "        img = arr.astype(np.uint8)\n",
    "\n",
    "    Image.fromarray(img).save(str(outpath))\n",
    "\n",
    "\n",
    "def overlay_image(image_np, mask_np, alpha=0.5, color=(255, 0, 0)):\n",
    "    \"\"\"Overlay mask (H,W) on image (H,W,3) and return uint8 image.\n",
    "\n",
    "    This function is defensive: it will squeeze batch/channel dims on the mask\n",
    "    and will resize the mask to match the image if necessary.\n",
    "    \"\"\"\n",
    "    img = np.array(image_np).astype(np.float32)\n",
    "    mask = np.array(mask_np)\n",
    "\n",
    "    # Squeeze common singletons (B,1,H,W) -> (H,W)\n",
    "    if mask.ndim == 4:\n",
    "        mask = mask[0]\n",
    "    if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "        mask = mask[0]\n",
    "    if mask.ndim == 3 and mask.shape[2] == 1:\n",
    "        mask = mask[..., 0]\n",
    "\n",
    "    # If mask is channel-first (C,H,W) where C>1, try to reduce to single channel\n",
    "    if mask.ndim == 3 and mask.shape[0] > 1 and mask.shape[0] <= 4:\n",
    "        mask = mask[0]\n",
    "\n",
    "    # Ensure mask is 2D now\n",
    "    if mask.ndim != 2:\n",
    "        raise ValueError(f\"overlay_image: expected 2D mask but got shape {mask.shape}\")\n",
    "\n",
    "    # If image has no channel dimension, expand\n",
    "    if img.ndim == 2:\n",
    "        img = np.stack([img] * 3, axis=-1)\n",
    "\n",
    "    # Resize mask if necessary\n",
    "    ih, iw = img.shape[:2]\n",
    "    mh, mw = mask.shape[:2]\n",
    "    if (ih, iw) != (mh, mw):\n",
    "        mask_img = Image.fromarray((np.clip(mask, 0, 1) * 255).astype(np.uint8))\n",
    "        mask_img = mask_img.resize((iw, ih), resample=Image.NEAREST)\n",
    "        mask = np.array(mask_img).astype(np.float32) / 255.0\n",
    "\n",
    "    # Ensure mask is 0..1 float\n",
    "    if mask.dtype != np.float32 and mask.dtype != np.float64:\n",
    "        mask = mask.astype(np.float32) / 255.0 if mask.max() > 1 else mask.astype(np.float32)\n",
    "\n",
    "    mask_2d = mask[..., None]\n",
    "    mask_col = np.zeros_like(img, dtype=np.float32)\n",
    "    for c in range(3):\n",
    "        mask_col[..., c] = mask_2d[..., 0] * float(color[c])\n",
    "\n",
    "    out = img * (1 - alpha) + mask_col * alpha\n",
    "    out = np.clip(out, 0, 255).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def infer_batch(inputs):\n",
    "    \"\"\"Run model and segmentation head to produce probability masks [B,1,H,W].\n",
    "\n",
    "    This function also validates the seg_head output shape and upsamples if needed.\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    seg_head.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = net(inputs)\n",
    "        # net may return tuple/list\n",
    "        if isinstance(outputs, (list, tuple)) and len(outputs) >= 3:\n",
    "            _, _, fused_feat = outputs\n",
    "        else:\n",
    "            # fallback: assume model returns fused feature directly\n",
    "            fused_feat = outputs\n",
    "        seg_pred = torch.sigmoid(seg_head(fused_feat))\n",
    "\n",
    "        # seg_pred should be [B,1,H,W]; if H/W are small, upsample to target 224x224\n",
    "        if seg_pred.dim() == 4 and (seg_pred.shape[2] < 32 or seg_pred.shape[3] < 32):\n",
    "            seg_pred = nn.functional.interpolate(seg_pred, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    return seg_pred.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100d0d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f68fe0bc81d4522b89c7ca05e1cacd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43minfer_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B,1,H,W]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m probs_np \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# [B,H,W]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m targets_np \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(targets, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(targets)\n",
      "Cell \u001b[0;32mIn[5], line 138\u001b[0m, in \u001b[0;36minfer_batch\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    137\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 138\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# net may return tuple/list\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m~/23b6034/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/23b6034/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/23b6034/MHNet/danial/model.py:55\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m prm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprm(f3_conv, spg_out)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# PreFCS processing\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m roi_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_fcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf3_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprm_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m branch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_fcs\u001b[38;5;241m.\u001b[39mbranch\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# CRFCS processing\u001b[39;00m\n",
      "File \u001b[0;32m~/23b6034/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/23b6034/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mfix_pre_fcs_sizes.<locals>.new_forward\u001b[0;34m(self, feature3, prm_out)\u001b[0m\n\u001b[1;32m     14\u001b[0m     prm_out_4d \u001b[38;5;241m=\u001b[39m prm_out\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 3. Force both to target size (224,224)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m f3_up \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39minterpolate(f3, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m prm_up \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(prm_out_4d, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 4. Element-wise addition (now sizes match)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# Section 5: Metrics and Batch Validation Loop\n",
    "\n",
    "import csv\n",
    "\n",
    "def iou_score(pred_bin, gt_bin, eps=1e-7):\n",
    "    pred = pred_bin.astype(bool)\n",
    "    gt = gt_bin.astype(bool)\n",
    "    inter = np.logical_and(pred, gt).sum()\n",
    "    union = np.logical_or(pred, gt).sum()\n",
    "    if union == 0:\n",
    "        return 1.0 if inter == 0 else 0.0\n",
    "    return float(inter) / (union + eps)\n",
    "\n",
    "\n",
    "def f1_score(pred_bin, gt_bin, eps=1e-7):\n",
    "    pred = pred_bin.astype(bool).sum()\n",
    "    gt = gt_bin.astype(bool).sum()\n",
    "    tp = np.logical_and(pred_bin, gt_bin).sum()\n",
    "    # precision = tp / (pred + eps); recall = tp / (gt + eps)\n",
    "    prec = tp / (pred + eps)\n",
    "    rec = tp / (gt + eps)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec + eps)\n",
    "\n",
    "\n",
    "def mae_score(pred, gt):\n",
    "    return float(np.abs(pred.astype(float) - gt.astype(float)).mean())\n",
    "\n",
    "# Run validation loop and save per-image metrics\n",
    "results = []\n",
    "net.eval(); seg_head.eval()\n",
    "\n",
    "pbar = tqdm(enumerate(val_loader), total=len(val_loader), desc='Validation')\n",
    "for batch_idx, batch in pbar:\n",
    "    try:\n",
    "        inputs, targets = batch\n",
    "    except Exception:\n",
    "        # If dataset returns different structure, try unpacking\n",
    "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "            inputs, targets = batch[0], batch[1]\n",
    "        else:\n",
    "            raise\n",
    "    probs = infer_batch(inputs)  # [B,1,H,W]\n",
    "    probs_np = probs.squeeze(1).numpy()  # [B,H,W]\n",
    "    targets_np = targets.squeeze(1).cpu().numpy() if isinstance(targets, torch.Tensor) else np.array(targets)\n",
    "\n",
    "    batch_size = probs_np.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        prob = probs_np[i]\n",
    "        gt = targets_np[i]\n",
    "        pred_bin = (prob >= THRESH).astype(np.uint8)\n",
    "        gt_bin = (gt >= 0.5).astype(np.uint8)\n",
    "        iou = iou_score(pred_bin, gt_bin)\n",
    "        f1 = f1_score(pred_bin, gt_bin)\n",
    "        mae = mae_score(prob, gt)\n",
    "        idx_global = batch_idx * BATCH_SIZE + i\n",
    "        # Try to derive a filename from dataset if possible\n",
    "        fname = None\n",
    "        try:\n",
    "            # many dataset implementations store image paths under .images or .image_paths\n",
    "            if hasattr(val_dataset, 'image_paths'):\n",
    "                fname = Path(val_dataset.image_paths[idx_global]).name\n",
    "            elif hasattr(val_dataset, 'images'):\n",
    "                fname = Path(val_dataset.images[idx_global]).name\n",
    "            elif hasattr(val_dataset, 'file_list'):\n",
    "                fname = Path(val_dataset.file_list[idx_global]).name\n",
    "        except Exception:\n",
    "            fname = f'img_{idx_global:06d}.png'\n",
    "        if fname is None:\n",
    "            fname = f'img_{idx_global:06d}.png'\n",
    "\n",
    "        # Save predicted mask and overlay image\n",
    "        try:\n",
    "            # Get original input image for visualization\n",
    "            inp_img = tensor_to_image(inputs[i])\n",
    "            mask_uint8 = (pred_bin * 255).astype(np.uint8)\n",
    "            mask_path = SAVE_RESULTS_DIR / 'images' / (Path(fname).stem + '_pred_mask.png')\n",
    "            overlay_path = SAVE_RESULTS_DIR / 'images' / (Path(fname).stem + '_overlay.png')\n",
    "            save_mask_image(mask_uint8, mask_path)\n",
    "            overlay = overlay_image(inp_img, pred_bin, alpha=0.4)\n",
    "            Image.fromarray(overlay).save(overlay_path)\n",
    "        except Exception as e:\n",
    "            # If saving fails, continue but note the failure\n",
    "            print('Warning: saving visualization failed for', fname, '-', e)\n",
    "\n",
    "        results.append({\n",
    "            'filename': str(fname),\n",
    "            'index': int(idx_global),\n",
    "            'iou': float(iou),\n",
    "            'f1': float(f1),\n",
    "            'mae': float(mae)\n",
    "        })\n",
    "\n",
    "# Save CSV\n",
    "metrics_csv = SAVE_RESULTS_DIR / 'metrics.csv'\n",
    "pd.DataFrame(results).to_csv(metrics_csv, index=False)\n",
    "\n",
    "# Print aggregated metrics\n",
    "if len(results) > 0:\n",
    "    df = pd.DataFrame(results)\n",
    "    print('Validation results saved to', metrics_csv)\n",
    "    print('Mean IoU: {:.4f}, Mean F1: {:.4f}, Mean MAE: {:.6f}'.format(df['iou'].mean(), df['f1'].mean(), df['mae'].mean()))\n",
    "else:\n",
    "    print('No results produced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53936fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug shapes in forward pass\n",
    "with torch.no_grad():\n",
    "    # Run a single sample through to debug shapes\n",
    "    sample_input = next(iter(val_loader))[0][:1].to(DEVICE)  # take 1st image of batch\n",
    "    print('Input shape:', sample_input.shape)\n",
    "    \n",
    "    # Run through backbone (F3)\n",
    "    net.eval()\n",
    "    x = sample_input\n",
    "    f3_conv = net.f3(x)\n",
    "    print('F3 output shape:', f3_conv.shape)\n",
    "    \n",
    "    # Run through SPG and PRM\n",
    "    spg_out = net.spg(f3_conv)\n",
    "    print('SPG output shape:', spg_out.shape)\n",
    "    prm_out = net.prm(f3_conv, spg_out)\n",
    "    print('PRM output shape:', prm_out.shape)\n",
    "    \n",
    "    # Debug PreFCS internals\n",
    "    if hasattr(net.pre_fcs, 'interpolate'):\n",
    "        # If pre_fcs has interpolation, what size does it use?\n",
    "        print('PreFCS interpolation size:', getattr(net.pre_fcs, 'size', None))\n",
    "    \n",
    "    # Try to identify where size 64 and 8 come from\n",
    "    print('\\nSuspect tensors (looking for 64 or 8 in any dimension):')\n",
    "    for name, tensor in [('f3_conv', f3_conv), ('spg_out', spg_out), ('prm_out', prm_out)]:\n",
    "        if any(s in (64, 8) for s in tensor.shape):\n",
    "            print(f'{name}: {tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6baf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Visualize some predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "\n",
    "# Load CSV if needed\n",
    "metrics_path = SAVE_RESULTS_DIR / 'metrics.csv'\n",
    "if metrics_path.exists():\n",
    "    metrics_df = pd.read_csv(metrics_path)\n",
    "else:\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "\n",
    "# pick a few random examples\n",
    "n_show = 6\n",
    "if len(metrics_df) == 0:\n",
    "    print('No examples to show')\n",
    "else:\n",
    "    samples = metrics_df.sample(n=min(n_show, len(metrics_df)), random_state=42)\n",
    "    fig_rows = ceil(len(samples) / 3)\n",
    "    fig, axes = plt.subplots(fig_rows, 3, figsize=(15, 5 * fig_rows))\n",
    "    axes = axes.flatten()\n",
    "    for ax, (_, row) in zip(axes, samples.iterrows()):\n",
    "        fname = row['filename']\n",
    "        stem = Path(fname).stem\n",
    "        # attempt to find files saved earlier\n",
    "        orig_idx = int(row.get('index', 0))\n",
    "        try:\n",
    "            # Try to open original input via dataset if path available\n",
    "            if hasattr(val_dataset, 'image_paths'):\n",
    "                img_path = val_dataset.image_paths[orig_idx]\n",
    "                orig = np.array(Image.open(img_path).convert('RGB').resize((224,224)))\n",
    "            else:\n",
    "                # fallback to using saved overlay\n",
    "                overlay_path = SAVE_RESULTS_DIR / 'images' / (stem + '_overlay.png')\n",
    "                orig = np.array(Image.open(overlay_path).convert('RGB'))\n",
    "        except Exception:\n",
    "            orig = np.zeros((224,224,3), dtype=np.uint8)\n",
    "        try:\n",
    "            gt_path = SAVE_RESULTS_DIR / 'images' / (stem + '_gt.png')\n",
    "            # we didn't save ground-truth by default; try to get from dataset\n",
    "            if hasattr(val_dataset, 'mask_paths'):\n",
    "                mask_path = val_dataset.mask_paths[orig_idx]\n",
    "                gt = Image.open(mask_path).convert('L').resize((224,224))\n",
    "                gt = np.array(gt)\n",
    "            else:\n",
    "                gt = None\n",
    "        except Exception:\n",
    "            gt = None\n",
    "        try:\n",
    "            pred_path = SAVE_RESULTS_DIR / 'images' / (stem + '_pred_mask.png')\n",
    "            pred = np.array(Image.open(pred_path).convert('L'))\n",
    "        except Exception:\n",
    "            pred = None\n",
    "\n",
    "        ax.imshow(orig)\n",
    "        title = f\"{fname}\\nIoU={row['iou']:.3f} F1={row['f1']:.3f} MAE={row['mae']:.4f}\"\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "        # if pred available, overlay a contour\n",
    "        if pred is not None:\n",
    "            # show a translucent mask overlay\n",
    "            overlay = overlay_image(orig, (pred>127).astype(np.uint8), alpha=0.35)\n",
    "            ax.imshow(overlay)\n",
    "    # hide unused axes\n",
    "    for j in range(len(samples), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e129dbe",
   "metadata": {},
   "source": [
    "# Section 7: Notes & How to run\n",
    "\n",
    "# Quick usage notes:\n",
    "# 1) Open this notebook in Jupyter or VS Code.\n",
    "# 2) Adjust `MODEL_DIR` and `CHECKPOINT` if you want to use a different checkpoint (e.g. 'last.pth' or 'checkpoint_epoch_600.pth').\n",
    "# 3) Change BATCH_SIZE if GPU memory is limited.\n",
    "# 4) If you want to save ground-truth crops/visualizations, add saving logic in the validation loop.\n",
    "\n",
    "print('Notebook ready. Run cells top-to-bottom to perform validation.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vengeance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
