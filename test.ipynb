{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f3d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from danial import model\n",
    "from danial import dataloader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import GaussianBlur, Normalize, ToTensor\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe13bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dania/code/fyp/MHNet/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/dania/code/fyp/MHNet/vengeance/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "mod = model.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0270a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from models/first600/best.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (backbone): VGGFeatureExtractor(\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block4): Sequential(\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block5): Sequential(\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (spg): SelectiveWeightedAttention(\n",
       "    (gr1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (gr2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (gr_wd): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fuse_conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fuse_conv2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (out_conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (gaussian): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (conv_f3): F3Conv(\n",
       "    (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (prm): PRM(\n",
       "    (conv_spg): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (primary): PrimaryCaps(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (conv_caps): ConvCaps()\n",
       "    (deconv_caps): DeconvCaps(\n",
       "      (deconv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pre_fcs): PreFCS(\n",
       "    (feature3_conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (attention): ReverseAttention(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (rpn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (roi_pool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (crfcs): CRFCS(\n",
       "    (mask_conv): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (boundary_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (recovery_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (attention_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (sigmoid): Sigmoid()\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (cls_head): Linear(in_features=3136, out_features=5, bias=True)\n",
       "    (reg_head): Linear(in_features=3136, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained weights from models/first600 (robust to common checkpoint formats)\n",
    "checkpoint_path = \"models/first600/best.pth\"\n",
    "try:\n",
    "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "    # checkpoint may be a dict with 'state_dict' or may be the state_dict itself\n",
    "    if isinstance(ckpt, dict) and 'state_dict' in ckpt:\n",
    "        state = ckpt['state_dict']\n",
    "    elif isinstance(ckpt, dict) and all(not isinstance(v, dict) for v in ckpt.values()):\n",
    "        # likely already a state_dict-like mapping\n",
    "        state = ckpt\n",
    "    else:\n",
    "        state = ckpt\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load checkpoint {checkpoint_path}: {e}\")\n",
    "    state = None\n",
    "if state is not None:\n",
    "    from collections import OrderedDict\n",
    "    new_state = OrderedDict()\n",
    "    for k, v in state.items():\n",
    "        # remove DataParallel/DistributedDataParallel 'module.' prefix if present\n",
    "        new_k = k.replace('module.', '') if k.startswith('module.') else k\n",
    "        new_state[new_k] = v\n",
    "    try:\n",
    "        mod.load_state_dict(new_state, strict=False)\n",
    "        print(f\"Loaded weights from {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load state dict into model: {e}\")\n",
    "mod.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "441c5702",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 112 but got size 56 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mload_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets/camo-sold-d.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     binary_pred, boundary_pred, fused_feat \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m avg_map \u001b[38;5;241m=\u001b[39m fused_feat\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [1, 7, 7]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(avg_map[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/fyp/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/fyp/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/fyp/MHNet/danial/model.py:46\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m f3 \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# SPG processing\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m spg_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# F3 processing\u001b[39;00m\n\u001b[1;32m     49\u001b[0m f3_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_f3(f3)\n",
      "File \u001b[0;32m~/code/fyp/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/fyp/MHNet/vengeance/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/fyp/MHNet/danial/spg.py:112\u001b[0m, in \u001b[0;36mSPGWithFPN.forward\u001b[0;34m(self, F1, F2)\u001b[0m\n\u001b[1;32m    110\u001b[0m p1_up \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(p1, size\u001b[38;5;241m=\u001b[39m(h, w), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    111\u001b[0m p3_down \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_max_pool2d(p3, (h, w))\n\u001b[0;32m--> 112\u001b[0m prior \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp1_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp3_down\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m prior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_conv(prior)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prior\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 112 but got size 56 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "test = dataloader.load_image(\"assets/camo-sold-d.png\", target_size = (224,224))\n",
    "with torch.no_grad():\n",
    "    binary_pred, boundary_pred, fused_feat = mod(test)\n",
    "\n",
    "avg_map = fused_feat.mean(dim=1)  # [1, 7, 7]\n",
    "\n",
    "plt.imshow(avg_map[0].detach().cpu(), cmap='jet')\n",
    "plt.title(\"Averaged Feature Map\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "avg_map = fused_feat.mean(dim=1, keepdim=True)  # [1, 1, 7, 7]\n",
    "\n",
    "# 1️⃣ Upsample to higher resolution (e.g., 224x224)\n",
    "upsampled = F.interpolate(avg_map, size=(448,448), mode='bilinear', align_corners=True)\n",
    "\n",
    "# 2️⃣ Apply Gaussian blur\n",
    "blur = GaussianBlur(kernel_size=(7,7), sigma=(1.0,1.0))\n",
    "smoothed = blur(upsampled)\n",
    "\n",
    "# 3️⃣ Show result\n",
    "plt.imshow(smoothed[0, 0].detach().cpu(), cmap='jet')  # using 'jet' for heatmap look\n",
    "plt.title(\"Smoothed & Upsampled Feature Map\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab978bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5]) torch.Size([1, 4]) torch.Size([1, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(binary_pred.shape, boundary_pred.shape, fused_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fa2bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def transgender():\n",
    "    # Load your RGB PNG and convert to grayscale\n",
    "    gt_image = Image.open('assets/COD10K-CAM-3-Flying-61-Katydid-4237.png').convert('L')  # 'L' = grayscale\n",
    "\n",
    "    # Convert to tensor [1, 1, 224, 224]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    gt_tensor = transform(gt_image).unsqueeze(0)  # Add batch dimension\n",
    "    return gt_tensor\n",
    "    # print(gt_tensor.shape)\n",
    "    # # Now both are [1, 1, 224, 224] - calculate metrics!\n",
    "    # metrics = ImageMetrics(gt_tensor, model_output, metric_type='reconstruction')\n",
    "    # print(metrics)\n",
    "\n",
    "gt = transgender()\n",
    "print(gt.shape)\n",
    "print(smoothed.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e41b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "IMAGE RECONSTRUCTION METRICS\n",
      "==================================================\n",
      "MSE (Mean Squared Error):     0.076334\n",
      "MAE (Mean Absolute Error):    0.091663\n",
      "PSNR (Peak Signal-to-Noise):  11.17 dB\n",
      "SSIM (Gaussian-weighted):     0.5101\n",
      "Correlation Coefficient:      0.2813\n",
      "Precision (continuous):       0.1300\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def rgb_to_grayscale(rgb_tensor):\n",
    "    \"\"\"Convert RGB image to grayscale using luminosity method.\"\"\"\n",
    "    if rgb_tensor.shape[1] == 3:  # [B, C, H, W]\n",
    "        weights = torch.tensor([0.299, 0.587, 0.114], device=rgb_tensor.device).view(1, 3, 1, 1)\n",
    "        gray = (rgb_tensor * weights).sum(dim=1, keepdim=True)\n",
    "    elif rgb_tensor.shape[-1] == 3:  # [B, H, W, C]\n",
    "        weights = torch.tensor([0.299, 0.587, 0.114], device=rgb_tensor.device)\n",
    "        gray = (rgb_tensor * weights).sum(dim=-1, keepdim=True)\n",
    "        gray = gray.permute(0, 3, 1, 2)  # [B, 1, H, W]\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 3 channels, got shape {rgb_tensor.shape}\")\n",
    "    return gray\n",
    "\n",
    "def gaussian_window(window_size=11, sigma=1.5, device='cpu'):\n",
    "    \"\"\"Create a 2D Gaussian kernel.\"\"\"\n",
    "    coords = torch.arange(window_size, dtype=torch.float32) - window_size // 2\n",
    "    g = torch.exp(-(coords**2) / (2 * sigma**2))\n",
    "    g = g / g.sum()\n",
    "    gauss = g[:, None] * g[None, :]\n",
    "    return gauss.to(device)\n",
    "\n",
    "class ImageMetrics:\n",
    "    \"\"\"Compute reconstruction metrics with Gaussian-weighted SSIM and precision.\"\"\"\n",
    "    \n",
    "    def __init__(self, gt, pred):\n",
    "        # Convert to tensor if needed\n",
    "        if not torch.is_tensor(gt):\n",
    "            gt = torch.tensor(gt)\n",
    "        if not torch.is_tensor(pred):\n",
    "            pred = torch.tensor(pred)\n",
    "        \n",
    "        # Convert RGB to grayscale if needed\n",
    "        if gt.shape[1] == 3 and pred.shape[1] == 1:\n",
    "            gt = rgb_to_grayscale(gt)\n",
    "        elif gt.shape[1] == 1 and pred.shape[1] == 3:\n",
    "            pred = rgb_to_grayscale(pred)\n",
    "        \n",
    "        # Validate shapes\n",
    "        if gt.shape != pred.shape:\n",
    "            raise ValueError(f\"Shape mismatch! gt: {gt.shape}, pred: {pred.shape}\")\n",
    "        \n",
    "        self.gt = gt.float()\n",
    "        self.pred = pred.float()\n",
    "        \n",
    "        # Compute metrics\n",
    "        self._calculate_metrics()\n",
    "    \n",
    "    def _calculate_metrics(self):\n",
    "        # MSE\n",
    "        self.mse = F.mse_loss(self.pred, self.gt).item()\n",
    "        \n",
    "        # MAE\n",
    "        self.mae = F.l1_loss(self.pred, self.gt).item()\n",
    "        \n",
    "        # PSNR\n",
    "        max_pixel = 1.0 if self.gt.max() <= 1.0 else 255.0\n",
    "        self.psnr = 10 * np.log10((max_pixel ** 2) / self.mse) if self.mse > 0 else float('inf')\n",
    "        \n",
    "        # Correlation\n",
    "        gt_flat = self.gt.flatten()\n",
    "        pred_flat = self.pred.flatten()\n",
    "        self.correlation = torch.corrcoef(torch.stack([gt_flat, pred_flat]))[0, 1].item()\n",
    "        \n",
    "        # Gaussian-weighted SSIM\n",
    "        self.ssim = self._calculate_gaussian_ssim()\n",
    "        \n",
    "        # \"Precision\" for continuous values (like ratio of correctly predicted intensity)\n",
    "        eps = 1e-8\n",
    "        self.precision = (torch.min(self.pred, self.gt).sum() / (self.pred.sum() + eps)).item()\n",
    "    \n",
    "    def _calculate_gaussian_ssim(self, window_size=11, sigma=1.5):\n",
    "        \"\"\"Compute SSIM using Gaussian window.\"\"\"\n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "        \n",
    "        device = self.gt.device\n",
    "        window = gaussian_window(window_size, sigma, device).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        mu1 = F.conv2d(self.gt, window, padding=window_size//2, groups=1)\n",
    "        mu2 = F.conv2d(self.pred, window, padding=window_size//2, groups=1)\n",
    "        \n",
    "        mu1_sq = mu1 ** 2\n",
    "        mu2_sq = mu2 ** 2\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "        \n",
    "        sigma1_sq = F.conv2d(self.gt**2, window, padding=window_size//2, groups=1) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(self.pred**2, window, padding=window_size//2, groups=1) - mu2_sq\n",
    "        sigma12 = F.conv2d(self.gt*self.pred, window, padding=window_size//2, groups=1) - mu1_mu2\n",
    "        \n",
    "        ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2)) / ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "        return ssim_map.mean().item()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{'='*50}\\n\"\n",
    "            f\"IMAGE RECONSTRUCTION METRICS\\n\"\n",
    "            f\"{'='*50}\\n\"\n",
    "            f\"MSE (Mean Squared Error):     {self.mse:.6f}\\n\"\n",
    "            f\"MAE (Mean Absolute Error):    {self.mae:.6f}\\n\"\n",
    "            f\"PSNR (Peak Signal-to-Noise):  {self.psnr:.2f} dB\\n\"\n",
    "            f\"SSIM (Gaussian-weighted):     {self.ssim:.4f}\\n\"\n",
    "            f\"Correlation Coefficient:      {self.correlation:.4f}\\n\"\n",
    "            f\"Precision (continuous):       {self.precision:.4f}\"\n",
    "        )\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'mse': self.mse,\n",
    "            'mae': self.mae,\n",
    "            'psnr': self.psnr,\n",
    "            'ssim': self.ssim,\n",
    "            'correlation': self.correlation,\n",
    "            'precision': self.precision\n",
    "        }\n",
    "\n",
    "\n",
    "metrics = ImageMetrics(gt, smoothed)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483cb5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50800144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 533)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_resize_show(image_path, size=(224,224)):\n",
    "    \"\"\"\n",
    "    Loads an image from the given path, resizes it to `size`, and displays it.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        size (tuple): Target size as (width, height). Default is (512, 512).\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    print(img.size)\n",
    "    \n",
    "    # Resize the image\n",
    "    img_resized = img.resize(size)\n",
    "    \n",
    "    # Show the image\n",
    "    img_resized.show()\n",
    "\n",
    "# Example usage\n",
    "load_resize_show(\"assets/COD10K-CAM-1-Aquatic-1-BatFish-2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e7ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vengeance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
